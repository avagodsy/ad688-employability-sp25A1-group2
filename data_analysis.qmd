---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends"
author:
  - name: Mahira Ayub
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
  - name: Ava Godsy
    affiliations:
      - ref: bu
  - name: Joshua Lawrence
    affiliations:
      - ref: bu
bibliography: references.bib
csl: csl/econometrica.csl
format: 
  html:
    theme: minty
    toc: true
    number-sections: true
    df-print: paged
---

# Exploratory Data Analysis 

## Importing Dataset Using Pandas
```{python}
import pandas as pd

# Load the dataset
data = pd.read_csv('data\lightcast_job_postings.csv')
```

## Dropping Unncessary Columns

# Answer the Questions

# Which columns are irrelevant or redundant?
# ID, URL ACTIVE_URLS, DUPLICATES, LAST_UPDATED TIMESTAMP are irrelevant or redundant columns. They are mostly used for internal tracking and don't contribute to the actual analysis of jobs, industries and occupations.

# Why are we removing multiple versions of NAICS/SOC codes?
# The dataset contains multiple versions of industry NAICS and Occupational SOC which can be risky to keep, as there is risk of duplications and inconsistent groupings.
  
# How will this improve analysis? 
# This will improve the analysis by enhancing the efficiency of the data; by having smaller datasets it will be easier to process and run data. It will also improve consistency because by having only one version of the data the risk of duplication will be low.

```{python}
columns_to_drop = [
'LAST_UPDATED_DATE', 'LAST_UPDATED_TIMESTAMP', 'DUPLICATES', 'EXPIRED', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW','BODY',
'COMPANY_NAME', 'COMPANY_RAW', 'COMPANY_IS_STAFFING', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'MIN_EDULEVELS',
'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'IS_INTERNSHIP',
'ORIGINAL_PAY_PERIOD', 'SALARY_TO', 'SALARY_FROM', 'COUNTY_OUTGOING', 'COUNTY_NAME_OUTGOING', 'COUNTY_INCOMING', 'COUNTY_NAME_INCOMING',
'MSA_OUTGOING', 'MSA_NAME_OUTGOING','MSA_INCOMING', 'MSA_NAME_INCOMING', 'NAICS2', 'NAICS2_NAME', 'NAICS3', 'NAICS3_NAME', 'NAICS4','NAICS4_NAME',
'NAICS5', 'NAICS5_NAME', 'NAICS6', 'NAICS6_NAME', 'ONET', 'ONET_NAME', 'ONET_2019', 'ONET_2019_NAME', 'CIP6', 'CIP6_NAME', 'CIP4', 'CIP4_NAME',
'CIP2', 'CIP2_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME', 'SOC_2021_3', 'SOC_2021_3_NAME', 'SOC_2021_4', 'SOC_2021_4_NAME', 'LOT_OCCUPATION_GROUP_NAME',
'LOT_V6_SPECIALIZED_OCCUPATION', 'LOT_V6_SPECIALIZED_OCCUPATION_NAME', 'LOT_V6_OCCUPATION', 'LOT_V6_OCCUPATION_NAME', 'LOT_V6_OCCUPATION_GROUP',
'LOT_V6_OCCUPATION_GROUP_NAME', 'LOT_V6_CAREER_AREA', 'LOT_V6_CAREER_AREA_NAME', 'SOC_2', 'SOC_2_NAME', 'SOC_3', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME',
'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3', 'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME', 'NAICS_2022_5', 'NAICS_2022_5_NAME',
'DURATION', 'SOURCE_TYPES', 'SOURCES', 'URL', 'ACTIVE_URLS', 'MODELED_EXPIRED', 'MODELED_DURATION', 'LOT_CAREER_AREA', 'LOT_CAREER_AREA_NAME',
'LOT_OCCUPATION', 'LOT_OCCUPATION_NAME', 'LOT_SPECIALIZED_OCCUPATION', 'LOT_SPECIALIZED_OCCUPATION_NAME', 'LOT_OCCUPATION_GROUP', 'LIGHTCAST_SECTORS',
'LIGHTCAST_SECTORS_NAME'
]
data.drop(columns=columns_to_drop, inplace=True)
```

## Handling Missing Values

#  Answer the question: How should missing values be handled? 

# Different strategies are used to handle missing values:
# - Numerical fields (e.g., Salary) are filled with the median.
# - Categorical fields (e.g., Industry) are replaced with "Unknown".
# - Columns with >50% missing values are dropped.

```{python}
import missingno as msno
import matplotlib.pyplot as plt



# Fill missing values
data["SALARY"].fillna(data["SALARY"].median(), inplace=True)
data["NAICS_2022_6_NAME"].fillna("Unknown", inplace=True)
data["REMOTE_TYPE_NAME"].fillna("None, inplace=True")

data.rename(columns={'NAICS_2022_6_NAME': 'INDUSTRY'}, inplace=True)

# Visualize missing data
msno.heatmap(data)
plt.title("Missing Values Heatmap")
plt.show()
```

# The heatmap shows a few missing values in the dataset. In this dataset most fields cluster near 1. The value 1 (dark blue) means the two columns have missing values together. 0.0 value (white) suggest there is no relationship.

## Remove Duplicates 

```{python}
data = data.drop_duplicates(subset=["TITLE", "COMPANY", "LOCATION", "POSTED"], keep="first")
```

## Job Postings by Industry

```{python}
data["posting_count"] = data["ID"]. groupby(data["INDUSTRY"]).transform("count")

industry_summary = data.groupby("INDUSTRY")["posting_count"].first().sort_values(ascending=False).head(25)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 10))
sns.barplot(x=industry_summary.values, y=industry_summary.index, orient='h')
plt.title("Top 25 Job Postings by Industry", fontsize=16, pad=20)
plt.xlabel("Number of Job Postings", fontsize=12)
plt.ylabel("Industry", fontsize=12)
plt.tight_layout()
plt.show()
```

### Job Posting Anaylsis 

## Salary Distribution by Industry

```{python}
data["AVERAGE_INDUSTRY_SALARY"] = data["SALARY"]. groupby(data["INDUSTRY"]).transform("mean").round()

top_40_industries = data.groupby("INDUSTRY")["AVERAGE_INDUSTRY_SALARY"].first().sort_values(ascending=False).head(40).index
filtered_data = data[data["INDUSTRY"].isin(top_40_industries)]

plt.figure(figsize=(20, 10))
sns.boxplot(data=filtered_data, x="INDUSTRY", y="SALARY", 
            order=top_40_industries)
plt.title("Salary Distribution by Industry (Top 40 by Average Salary)", fontsize=16, pad=20)
plt.xlabel("Industry", fontsize=12)
plt.ylabel("Salary", fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
```

### Salary Analysis

## Remote vs. On-Site Jobs

```{python}
remote_counts = data["REMOTE_TYPE_NAME"].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(remote_counts.values, labels=remote_counts.index, autopct='%1.1f%%')
plt.title("Remote vs. On-Site Jobs", fontsize=16, pad=20)
plt.show()
```

#  Why these visualizations were chosen:

# - We chose a bar chart for the 'Job Postings by Industry' because it makes it easy to compare job postings across different industries. Bars provides a clear ranking which is easier to interpret.
# - For the 'Salary Distribution by Industry' we chose a Boxplot because it can show medians, outliers and summarize salary distributions. It allows for comparison between industries and detect an variability in salaries.
# - The Pie Chart provides a clear visual of how Remote and On-site jobs are divided within a dataset.


#  Key insights from each graph:
  
# - The bar chart ranks the top 25 industries by job-posting volume. The chart has a skewed distribution, and it shows that demand is concentrated in technology and professional services. A huge chunk of 'unclassified industry' job postings suggest that there are a large number of jobs posted which are not mapped to any industry.

# - The boxplot of 'Salary Distribution by Industry' suggest that salaries across the 40 industries skew high, with medians generally in $120k to $170k range. The industries that have higher medians and greater dispersions includes software/ semiconductors, and consulting. Several industries have wide IQRs and many upper outliers. Similarly, some industries have less variability in pay which means that the pay is more standardized.

# - The pie chart shows that 17% remote jobs, 3.1% are hybrid jobs and 1.6% are not remote jobs and large number of job postings do not specify if the job is remote or on-site. 