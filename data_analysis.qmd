---
title: "Data Analysis"
subtitle: "Comprehensive Data Cleaning & Exploratory Analysis of Job Market Trends"
author:
  - name: Mahira Ayub
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
  - name: Ava Godsy
    affiliations:
      - ref: bu
  - name: Joshua Lawrence
    affiliations:
      - ref: bu
bibliography: references.bib
csl: csl/econometrica.csl
format: 
  html:
    toc: true
---

# Exploratory Data Analysis 

## Importing Dataset Using Pandas
```{python}

import pandas as pd

# Load the dataset
data = pd.read_csv('data\lightcast_job_postings.csv')
```

## Dropping Unncessary Columns

- **Which columns are irrelevant or redundant?**  
  ID, URL, ACTIVE_URLS, DUPLICATES, LAST_UPDATED TIMESTAMP are irrelevant or redundant columns. They are mostly used for internal tracking and don't contribute to the actual analysis of jobs, industries, and occupations.

- **Why are we removing multiple versions of NAICS/SOC codes?**  
  The dataset contains multiple versions of industry NAICS and Occupational SOC which can be risky to keep, as there is risk of duplications and inconsistent groupings.

- **How will this improve analysis?**  
  This will improve the analysis by enhancing the efficiency of the data; by having smaller datasets it will be easier to process and run data. It will also improve consistency because by having only one version of the data the risk of duplication will be low.


```{python}
columns_to_drop = [
'LAST_UPDATED_DATE', 'LAST_UPDATED_TIMESTAMP', 'DUPLICATES', 'EXPIRED', 'ACTIVE_SOURCES_INFO', 'TITLE_RAW','BODY',
'COMPANY_NAME', 'COMPANY_RAW', 'COMPANY_IS_STAFFING', 'EDUCATION_LEVELS', 'EDUCATION_LEVELS_NAME', 'MIN_EDULEVELS',
'MIN_EDULEVELS_NAME', 'MAX_EDULEVELS', 'MAX_EDULEVELS_NAME', 'MIN_YEARS_EXPERIENCE', 'MAX_YEARS_EXPERIENCE', 'IS_INTERNSHIP',
'ORIGINAL_PAY_PERIOD', 'SALARY_TO', 'SALARY_FROM', 'COUNTY_OUTGOING', 'COUNTY_NAME_OUTGOING', 'COUNTY_INCOMING', 'COUNTY_NAME_INCOMING',
'MSA_OUTGOING', 'MSA_NAME_OUTGOING','MSA_INCOMING', 'MSA_NAME_INCOMING', 'NAICS2', 'NAICS2_NAME', 'NAICS3', 'NAICS3_NAME', 'NAICS4','NAICS4_NAME',
'NAICS5', 'NAICS5_NAME', 'NAICS6', 'NAICS6_NAME', 'ONET', 'ONET_NAME', 'ONET_2019', 'ONET_2019_NAME', 'CIP6', 'CIP6_NAME', 'CIP4', 'CIP4_NAME',
'CIP2', 'CIP2_NAME', 'SOC_2021_2', 'SOC_2021_2_NAME', 'SOC_2021_3', 'SOC_2021_3_NAME', 'SOC_2021_4', 'SOC_2021_4_NAME', 'LOT_OCCUPATION_GROUP_NAME',
'LOT_V6_SPECIALIZED_OCCUPATION', 'LOT_V6_SPECIALIZED_OCCUPATION_NAME', 'LOT_V6_OCCUPATION', 'LOT_V6_OCCUPATION_NAME', 'LOT_V6_OCCUPATION_GROUP',
'LOT_V6_OCCUPATION_GROUP_NAME', 'LOT_V6_CAREER_AREA', 'LOT_V6_CAREER_AREA_NAME', 'SOC_2', 'SOC_2_NAME', 'SOC_3', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME',
'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3', 'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME', 'NAICS_2022_5', 'NAICS_2022_5_NAME',
'DURATION', 'SOURCE_TYPES', 'SOURCES', 'URL', 'ACTIVE_URLS', 'MODELED_EXPIRED', 'MODELED_DURATION', 'LOT_CAREER_AREA', 'LOT_CAREER_AREA_NAME',
'LOT_OCCUPATION', 'LOT_OCCUPATION_NAME', 'LOT_SPECIALIZED_OCCUPATION', 'LOT_SPECIALIZED_OCCUPATION_NAME', 'LOT_OCCUPATION_GROUP', 'LIGHTCAST_SECTORS',
'LIGHTCAST_SECTORS_NAME'
]
data.drop(columns=columns_to_drop, inplace=True)
```

## Handling Missing Values

- **Answer the question: How should missing values be handled?**

  - Numerical fields (e.g., Salary) are filled with the median.
  - Categorical fields (e.g., Industry) are replaced with "Unknown".
  - Columns with >50% missing values are dropped.

```{python}
import missingno as msno
import matplotlib.pyplot as plt



# Fill missing values
data["SALARY"].fillna(data["SALARY"].median(), inplace=True)
data["NAICS_2022_6_NAME"].fillna("Unknown", inplace=True)
data["REMOTE_TYPE_NAME"].fillna("None, inplace=True")

data.rename(columns={'NAICS_2022_6_NAME': 'INDUSTRY'}, inplace=True)
```
```{python}

data.dropna(thresh=len(data) * 0.5, axis=1, inplace=True)

# Visualize missing data
import matplotlib as mpl
from matplotlib.colors import LinearSegmentedColormap

# Set global font settings
mpl.rcParams['font.family'] = 'Verdana'
mpl.rcParams['font.size'] = 14
mpl.rcParams['text.color'] = 'black'
mpl.rcParams['axes.labelcolor'] = 'black'
mpl.rcParams['xtick.color'] = 'black'
mpl.rcParams['ytick.color'] = 'black'

# Define custom green-to-red colormap
custom_cmap = LinearSegmentedColormap.from_list(
    'custom_green_red', ['#B14E53', '#78C2AD'], N=256
)

# Create the heatmap
fig = plt.figure(figsize=(10, 6))
ax = msno.heatmap(data, cmap=custom_cmap)

# Set the title
plt.title("Missing Values Heatmap", fontsize=18, fontweight='bold')

plt.show()
plt.savefig("figures/missing_values.png")
```

The heatmap shows a few missing values in the dataset. In this dataset most fields cluster near 1. The value 1 (dark blue) means the two columns have missing values together. 0.0 value (white) suggest there is no relationship.

## Remove Duplicates 

```{python}
data = data.drop_duplicates(subset=["TITLE", "COMPANY", "LOCATION", "POSTED"], keep="first")
```

## Job Postings by Industry

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl

# Set global font family to Verdana
mpl.rcParams['font.family'] = 'Verdana'

# Compute posting count per industry
data["posting_count"] = data["ID"].groupby(data["INDUSTRY"]).transform("count")

# Summarize and sort top 25 industries
industry_summary = data.groupby("INDUSTRY")["posting_count"].first().sort_values(ascending=False).head(25)

# Plot
plt.figure(figsize=(12, 10))
sns.barplot(
    x=industry_summary.values,
    y=industry_summary.index,
    orient='h',
    color='#6CC3D5'
)

# Title and labels with updated font sizes
plt.title("Top 25 Job Postings by Industry", fontsize=18, fontweight='bold', pad=20)
plt.xlabel("Number of Job Postings", fontsize=14)
plt.ylabel("Industry", fontsize=14)

plt.tight_layout()
plt.show()
plt.savefig("figures/top_postings.png")
```

## Salary Distribution by Industry

```{python}
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib as mpl

# Set global font family to Verdana
mpl.rcParams['font.family'] = 'Verdana'

# Calculate average industry salary
data["AVERAGE_INDUSTRY_SALARY"] = data["SALARY"].groupby(data["INDUSTRY"]).transform("mean").round()

# Get top 40 industries by average salary
top_40_industries = data.groupby("INDUSTRY")["AVERAGE_INDUSTRY_SALARY"].first().sort_values(ascending=False).head(40).index
filtered_data = data[data["INDUSTRY"].isin(top_40_industries)]

# Create figure with taller height
plt.figure(figsize=(20, 14))  # Increased height from 10 to 14

# Create boxplot with custom color
sns.boxplot(
    data=filtered_data,
    x="INDUSTRY",
    y="SALARY",
    order=top_40_industries,
    color='#F3969A'  # Custom box color
)

# Title and labels with updated font sizes
plt.title("Salary Distribution by Industry (Top 40 by Average Salary)", fontsize=18, fontweight='bold', pad=20)
plt.xlabel("Industry", fontsize=14)
plt.ylabel("Salary", fontsize=14)

# Rotate x-tick labels for readability
plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()

plt.savefig("figures/salary_dist.png")

```

## Remote vs. On-Site Jobs

```{python}
import matplotlib.pyplot as plt
import matplotlib as mpl

# Set global font family to Verdana
mpl.rcParams['font.family'] = 'Verdana'

# Clean and group remote types
remote_grouped = data["REMOTE_TYPE_NAME"].fillna("Onsite").replace({
    "[None]": "Onsite", 
    "Not Remote": "Onsite",
    "None": "Onsite"
})

# Count values
remote_counts = remote_grouped.value_counts()

# Define colors matching labels (make sure the order matches remote_counts.index)
color_map = {
    "Onsite": "#297C8A",
    "Hybrid Remote": "#78C2AD",
    "Remote": "#F3969A"
}

# Get colors for the pie chart slices in the correct order, fallback to gray if missing
colors = [color_map.get(label, "#cccccc") for label in remote_counts.index]

# Plot pie chart
plt.figure(figsize=(8, 8))
wedges, texts, autotexts = plt.pie(
    remote_counts.values,
    labels=remote_counts.index,
    autopct='%1.1f%%',
    colors=colors,
    textprops={'fontsize': 14, 'color': 'black'}
)

# Title with larger font
plt.title("Remote vs. On-Site Jobs", fontsize=18, fontweight='bold', pad=20)

# Adjust autotext (percentage text) font size
for autotext in autotexts:
    autotext.set_fontsize(14)

plt.show()
plt.savefig("figures/remote_onsite.png")

```

## Why these visualizations were chosen

- **Bar Chart: Job Postings by Industry**  
  - Makes it easy to compare job postings across different industries.  
  - Provides a clear ranking that is simple to interpret.

- **Boxplot: Salary Distribution by Industry**  
  - Shows medians, outliers, and summarizes salary distributions.  
  - Allows for comparisons between industries and helps detect salary variability.

- **Pie Chart: Job Location Types**  
  - Provides a clear visual breakdown of remote versus on-site jobs within the dataset.

## Key insights from each graph

- **Bar Chart: Top 25 Industries by Job-Posting Volume**  
  - The distribution is skewed, showing demand concentrated in technology and professional services.  
  - A large portion of "unclassified industry" job postings suggests many jobs are not mapped to any industry.

- **Boxplot: Salary Distribution by Industry**  
  - Salaries across the 40 industries skew high, with medians generally in the $120kâ€“$170k range.  
  - Industries with higher medians and greater dispersions include software/semiconductors and consulting.  
  - Several industries have wide IQRs and many upper outliers, while some have less variability, indicating standardized pay.

- **Pie Chart: Job Location Types**  
  - 17% of jobs are remote, 3.1% are hybrid, and 1.6% are not remote.  
  - A large number of job postings do not specify whether the job is remote or on-site.